{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjU4x1d5d0zl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.data import Batch\n",
        "from torch_geometric.loader import DataLoader\n",
        "import gpytorch\n",
        "\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NN, self).__init__()\n",
        "        self.fc1 = nn.Linear(119, 360)\n",
        "        self.fc2 = nn.Linear(360, 180)\n",
        "        self.fc3 = nn.Linear(180, 100)\n",
        "        self.fc4 = nn.Linear(100, 64)\n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        self.fc6 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = torch.relu(self.fc5(x))\n",
        "        x = self.fc6(x)\n",
        "        return x\n",
        "model = NN()"
      ],
      "metadata": {
        "id": "nOjWWb9uh3pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, device, model, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    average_loss = running_loss / len(train_loader)\n",
        "    return average_loss\n",
        "\n",
        "def validate(validation_loader, device, model, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    validation_outputs = []\n",
        "    validation_truth = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            validation_outputs.append(outputs.detach().numpy())\n",
        "            validation_truth.append(labels.detach().numpy())\n",
        "\n",
        "    average_loss = running_loss / len(validation_loader)\n",
        "    validation_outputs = np.concatenate(validation_outputs)\n",
        "    validation_truth = np.concatenate(validation_truth)\n",
        "\n",
        "    return average_loss, validation_outputs, validation_truth\n",
        "\n",
        "\n",
        "def test(test_loader, device, model, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    test_outputs = []\n",
        "    test_truth = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            test_outputs.append(outputs.detach().numpy())\n",
        "            test_truth.append(targets.detach().numpy())\n",
        "\n",
        "    average_loss = running_loss / len(test_loader)\n",
        "    test_outputs = np.concatenate(test_outputs)\n",
        "    test_truth = np.concatenate(test_truth)\n",
        "\n",
        "    return average_loss, test_outputs, test_truth"
      ],
      "metadata": {
        "id": "RYA9nB5lh4Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_size = 6\n",
        "\n",
        "best_vals =[]\n",
        "best_models = []\n",
        "best_tests = []\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_test_split_index = int(0.9 * len(data_list))\n",
        "\n",
        "train_val_data = data_list[:train_test_split_index]\n",
        "\n",
        "test_data = data_list[train_test_split_index:]\n",
        "test_loader = Dataloader(test_data, batch_size=16, shuffle=False)\n",
        "\n",
        "data_len = len(train_val_data)\n",
        "num_batches = data_len // sampling_size\n",
        "indices = np.arange(data_len)\n",
        "for i in range(num_batches):\n",
        "    best_val_loss = np.inf\n",
        "\n",
        "    start_index = i * sampling_size\n",
        "\n",
        "    val_indices = indices[start_index:start_index + sampling_size]\n",
        "\n",
        "    train_indices = np.setdiff1d(indices, val_test_indices)\n",
        "\n",
        "    train_data = [train_val_data[j] for j in train_indices]\n",
        "    val_data = [train_val_data[j] for j in val_indices]\n",
        "\n",
        "    train_loader = Dataloader(train_data, batch_size=16, shuffle=True)\n",
        "    validation_loader = Dataloader(val_data, batch_size=16, shuffle=False)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                        factor=0.8, patience=5,\n",
        "                                                        min_lr=0.0000001)\n",
        "    criterion = nn.L1Loss()\n",
        "    best_validation_loss = float('inf')\n",
        "\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "\n",
        "        model.train()\n",
        "        loss = train(train_loader, device, model, optimizer, criterion)\n",
        "        scheduler.step(loss)\n",
        "\n",
        "        validation_loss, validation_output, validation_truth_temp = validate(validation_loader, device, model, criterion)\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}\")\n",
        "        if validation_loss < best_validation_loss:\n",
        "            best_validation_loss = validation_loss\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "            best_model = model\n",
        "            best_val_loss = validation_loss\n",
        "            print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
        "model.load_state_dict(best_model_state)\n",
        "test_loss, test_outputs, test_truth = test(test_loader, device, model, criterion)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "k8mFEPZ1gkfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dval = xgb.DMatrix(X_val, label=y_val)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'eval_metric': 'rmse',\n",
        "    'max_depth': 6,\n",
        "    'eta': 0.1,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8\n",
        "}\n",
        "\n",
        "evals = [(dtrain, 'train'), (dval, 'eval')]\n",
        "\n",
        "num_boost_round = 100\n",
        "early_stopping_rounds = 10\n",
        "model = xgb.train(params, dtrain, num_boost_round, evals, early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n",
        "\n",
        "y_pred = model.predict(dtest)\n",
        "test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "print(f'Test RMSE: {test_rmse:.4f}')"
      ],
      "metadata": {
        "id": "-_x4G9KvzDJh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}